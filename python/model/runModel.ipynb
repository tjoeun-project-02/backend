{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gljdUR1-TJQh",
        "outputId": "ca34e721-6378-4556-eed6-ce3e4e6f0511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keybert\n",
            "  Downloading keybert-0.9.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from keybert) (2.0.2)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.12/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.12/dist-packages (from keybert) (5.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.4.0->keybert) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.4.0->keybert) (2.19.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->keybert) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->keybert) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.57.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.7.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2026.1.4)\n",
            "Downloading keybert-0.9.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keybert\n",
            "Successfully installed keybert-0.9.0\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install keybert\n",
        "!pip install -q keybert torch pytorch-lightning transformers pandas scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHoPQ4UIbCDV",
        "outputId": "bfe32f04-1127-4eac-ad3b-e0e4788ffb58"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ë§› í”„ë¡œí•„, í‚¤ì›Œë“œ ì„¤ì •\n",
        "# [0] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„¤ì •\n",
        "# ==========================================\n",
        "# !pip install -q keybert torch pytorch-lightning transformers pandas scikit-learn\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import glob\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "from keybert import KeyBERT # í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬!\n",
        "\n",
        "# ==========================================\n",
        "# [1] ê²½ë¡œ ì„¤ì • (âš ï¸ ë³¸ì¸ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì • í•„ìˆ˜)\n",
        "# ==========================================\n",
        "# í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ (.ckpt) ê²½ë¡œ\n",
        "CHECKPOINT_PATH = '/content/drive/MyDrive/Tjoproject_model/models/checkpoints/distilbert-epoch=07-val_loss=0.0219.ckpt'\n",
        "\n",
        "# ì›ë³¸ JSON íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë”\n",
        "INPUT_FOLDER = '/content/drive/MyDrive/Tjoproject_model/whiskyData'\n",
        "\n",
        "# ê²°ê³¼ë¬¼ì„ ì €ì¥í•  ìƒˆë¡œìš´ í´ë”\n",
        "OUTPUT_FOLDER = '/content/drive/MyDrive/Tjoproject_model/processed_data'\n",
        "\n",
        "# ==========================================\n",
        "# [2] ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ í•„ìˆ˜)\n",
        "# ==========================================\n",
        "class WhiskyFlavorPredictor(pl.LightningModule):\n",
        "    def __init__(self, n_classes=6, lr=2e-5):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.bert = AutoModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = output.last_hidden_state[:, 0]\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "# ==========================================\n",
        "# [3] ë¶„ì„ í•¨ìˆ˜ ì •ì˜ (ìµœì¢… ì™„ì„±ë³¸: ws_id ì¶”ê°€ + êµ¬ì¡° ì •ë¦¬ + GPU ê°€ì†)\n",
        "# ==========================================\n",
        "def process_all_whiskies():\n",
        "    # 0. ë§› í‘œí˜„ ë‹¨ì–´ì¥ (AIê°€ ì´ ì¤‘ì—ì„œë§Œ í‚¤ì›Œë“œë¥¼ ì„ íƒí•¨)\n",
        "    FLAVOR_VOCAB = [\n",
        "        \"fruity\", \"fruit\", \"apple\", \"pear\", \"citrus\", \"lemon\", \"orange\", \"grape\", \"berry\", \"cherry\", \"plum\", \"peach\", \"apricot\", \"banana\", \"pineapple\", \"mango\", \"tropical\", \"dried fruit\", \"raisin\", \"fig\", \"date\", \"sultana\", \"prune\",\n",
        "        \"sweet\", \"honey\", \"vanilla\", \"caramel\", \"toffee\", \"butterscotch\", \"chocolate\", \"cocoa\", \"sugar\", \"syrup\", \"maple\", \"candy\", \"cream\", \"custard\", \"butter\", \"cake\", \"biscuit\",\n",
        "        \"spicy\", \"spice\", \"pepper\", \"cinnamon\", \"ginger\", \"clove\", \"nutmeg\", \"anise\", \"licorice\", \"mint\", \"herbal\",\n",
        "        \"peaty\", \"peat\", \"smoke\", \"smoky\", \"ash\", \"bonfire\", \"charcoal\", \"tar\", \"iodine\", \"medicinal\", \"seaweed\", \"salt\", \"brine\", \"maritime\",\n",
        "        \"woody\", \"oak\", \"wood\", \"pine\", \"cedar\", \"sawdust\", \"tannin\", \"dry\", \"nutty\", \"nut\", \"almond\", \"walnut\", \"hazelnut\", \"pecan\",\n",
        "        \"malty\", \"malt\", \"grain\", \"cereal\", \"barley\", \"bread\", \"toast\", \"yeast\", \"dough\",\n",
        "        \"floral\", \"flower\", \"rose\", \"heather\", \"grass\", \"hay\", \"straw\", \"leaf\", \"tea\", \"tobacco\", \"leather\", \"wax\", \"oil\", \"earthy\", \"mushroom\",\n",
        "        \"sherry\", \"bourbon\", \"port\", \"rum\", \"wine\", \"cask\", \"finish\", \"rich\", \"smooth\", \"complex\", \"balanced\"\n",
        "    ]\n",
        "\n",
        "    # í´ë” ìƒì„±\n",
        "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "    # 1. ëª¨ë¸ ë¡œë”©\n",
        "    print(\"â³ ëª¨ë¸ë“¤ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...\")\n",
        "    flavor_model = WhiskyFlavorPredictor.load_from_checkpoint(CHECKPOINT_PATH)\n",
        "    flavor_model.eval()\n",
        "    flavor_model.freeze()\n",
        "\n",
        "    # GPU ì¥ì¹˜ í™•ì¸ (ì¤‘ìš”)\n",
        "    device = flavor_model.device\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    kw_model = KeyBERT()\n",
        "    print(f\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ì‚¬ìš© ì¥ì¹˜: {device})\")\n",
        "\n",
        "    # 2. íŒŒì¼ ì²˜ë¦¬ ì‹œì‘\n",
        "    json_files = glob.glob(os.path.join(INPUT_FOLDER, \"*.json\"))\n",
        "    print(f\"\\nğŸ“‚ ì´ {len(json_files)}ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\\n\")\n",
        "\n",
        "    for file_path in json_files:\n",
        "        try:\n",
        "            file_name = os.path.basename(file_path)\n",
        "            save_path = os.path.join(OUTPUT_FOLDER, file_name)\n",
        "\n",
        "            # ì¤‘ë³µ ê±´ë„ˆë›°ê¸°\n",
        "            if os.path.exists(save_path):\n",
        "                print(f\"â© {file_name}: ì´ë¯¸ ì™„ë£Œëœ íŒŒì¼ì…ë‹ˆë‹¤. (Pass)\")\n",
        "                continue\n",
        "\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            reviews = data.get('reviews', [])\n",
        "            if not reviews:\n",
        "                print(f\"âš ï¸ {file_name}: ë¦¬ë·° ì—†ìŒ (Skip)\")\n",
        "                continue\n",
        "\n",
        "            print(f\"ğŸ¥ƒ ë¶„ì„ ì¤‘: {file_name} (ë¦¬ë·° {len(reviews)}ê°œ)...\", end=\"\")\n",
        "\n",
        "            # --- [Step 1] ë§› ì ìˆ˜ í”„ë¡œí•„ ê³„ì‚° ---\n",
        "            all_scores = []\n",
        "            full_text_corpus = []\n",
        "\n",
        "            for r in reviews:\n",
        "                parts = []\n",
        "                if isinstance(r, dict):\n",
        "                    if r.get('nose'): parts.append(str(r.get('nose')))\n",
        "                    if r.get('taste'): parts.append(str(r.get('taste')))\n",
        "                    if r.get('finish'): parts.append(str(r.get('finish')))\n",
        "                    if r.get('content'): parts.append(str(r.get('content')))\n",
        "                    text = \" \".join(parts)\n",
        "                else:\n",
        "                    text = str(r)\n",
        "\n",
        "                if len(text) < 10: continue\n",
        "                full_text_corpus.append(text)\n",
        "\n",
        "                inputs = tokenizer.encode_plus(\n",
        "                    text, add_special_tokens=True, max_length=256,\n",
        "                    padding=\"max_length\", truncation=True, return_tensors='pt'\n",
        "                )\n",
        "\n",
        "                # ğŸ”¥ ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™ (ì—ëŸ¬ ë°©ì§€)\n",
        "                input_ids = inputs['input_ids'].to(device)\n",
        "                attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    pred = flavor_model(input_ids, attention_mask)\n",
        "                    score = pred[0].cpu().numpy() * 10.0 # ê²°ê³¼ëŠ” ë‹¤ì‹œ CPUë¡œ\n",
        "                    all_scores.append(score)\n",
        "\n",
        "            # í‰ê·  ì ìˆ˜\n",
        "            if all_scores:\n",
        "                avg_scores = np.mean(all_scores, axis=0)\n",
        "                profile_dict = {\n",
        "                    \"fruity\": round(float(avg_scores[0]), 1),\n",
        "                    \"sweet\": round(float(avg_scores[1]), 1),\n",
        "                    \"peaty\": round(float(avg_scores[2]), 1),\n",
        "                    \"spicy\": round(float(avg_scores[3]), 1),\n",
        "                    \"woody\": round(float(avg_scores[4]), 1),\n",
        "                    \"malty\": round(float(avg_scores[5]), 1)\n",
        "                }\n",
        "            else:\n",
        "                profile_dict = {}\n",
        "\n",
        "            # --- [Step 2] ë§› í‚¤ì›Œë“œ ì¶”ì¶œ (ë‹¨ì–´ì¥ ê¸°ë°˜) ---\n",
        "            combined_text = \" \".join(full_text_corpus)\n",
        "            keywords = kw_model.extract_keywords(\n",
        "                combined_text,\n",
        "                candidates=FLAVOR_VOCAB, # ğŸŒŸ ë§› ë‹¨ì–´ì¥ ì ìš©\n",
        "                stop_words='english',\n",
        "                use_mmr=True,\n",
        "                diversity=0.7,\n",
        "                top_n=5\n",
        "            )\n",
        "            top_keywords = [kw[0] for kw in keywords]\n",
        "\n",
        "            # --- [Step 3] ìµœì¢… ë°ì´í„° êµ¬ì¡° ì •ë¦¬ ---\n",
        "\n",
        "            # 1. íŒŒì¼ëª…ì—ì„œ ìˆ«ì ID ì¶”ì¶œ (ì˜ˆ: wb_116460.json -> 116460)\n",
        "            # ìˆ«ìë§Œ ë‚¨ê¸°ëŠ” ì•ˆì „í•œ ë°©ì‹ ì‚¬ìš©\n",
        "            ws_id = \"\".join(filter(str.isdigit, file_name))\n",
        "\n",
        "            # 2. info ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬)\n",
        "            final_info = data.get('info', {})\n",
        "\n",
        "            # 3. ìƒˆë¡œìš´ ì •ë³´ë“¤(ID, ê°œìˆ˜, ë§›, í‚¤ì›Œë“œ)ì„ info ì•ˆì— í†µí•©\n",
        "            final_info['ws_id'] = ws_id\n",
        "\n",
        "            # collected_countê°€ ë°”ê¹¥ì— ìˆìœ¼ë©´ ì•ˆìœ¼ë¡œ ê°€ì ¸ì˜´\n",
        "            if 'collected_count' in data:\n",
        "                final_info['collected_count'] = data['collected_count']\n",
        "            elif 'collected_count' not in final_info:\n",
        "                final_info['collected_count'] = 0\n",
        "\n",
        "            final_info['flavor_profile'] = profile_dict\n",
        "            final_info['top_keywords'] = top_keywords\n",
        "\n",
        "            # 4. ìµœì¢… ì €ì¥ (infoë§Œ ì €ì¥)\n",
        "            final_data = {\"info\": final_info} # info í‚¤ë¡œ í•œ ë²ˆ ê°ì‹¸ì¤ë‹ˆë‹¤ (ìš”ì²­í•˜ì‹  í˜•ì‹ ìœ ì§€)\n",
        "\n",
        "            with open(save_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(final_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "            print(f\" -> âœ… ì™„ë£Œ!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâŒ ì—ëŸ¬ ë°œìƒ ({file_path}): {e}\")\n",
        "\n",
        "    print(f\"\\nğŸ‰ ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "    print(f\"ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜: {OUTPUT_FOLDER}\")\n",
        "# ==========================================\n",
        "# [ì‹¤í–‰]\n",
        "# ==========================================\n",
        "if __name__ == '__main__':\n",
        "    process_all_whiskies()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDQmKu11TMsa",
        "outputId": "2d72932e-a9cf-4f17-996a-3bcf5c900faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ ëª¨ë¸ë“¤ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...\n",
            "âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ì‚¬ìš© ì¥ì¹˜: cuda:0)\n",
            "\n",
            "ğŸ“‚ ì´ 49ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
            "\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_219297.json (ë¦¬ë·° 15ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_182815.json (ë¦¬ë·° 57ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_161049.json (ë¦¬ë·° 31ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_165510.json (ë¦¬ë·° 53ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_165512.json (ë¦¬ë·° 50ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_159713.json (ë¦¬ë·° 51ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_159714.json (ë¦¬ë·° 53ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_211.json (ë¦¬ë·° 53ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_2351.json (ë¦¬ë·° 31ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_193720.json (ë¦¬ë·° 41ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_208069.json (ë¦¬ë·° 50ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_248393.json (ë¦¬ë·° 23ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_32091.json (ë¦¬ë·° 51ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_142592.json (ë¦¬ë·° 53ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_145758.json (ë¦¬ë·° 33ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_113049.json (ë¦¬ë·° 22ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_158760.json (ë¦¬ë·° 34ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_5943.json (ë¦¬ë·° 52ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_82142.json (ë¦¬ë·° 10ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_76497.json (ë¦¬ë·° 57ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_71595.json (ë¦¬ë·° 37ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_239.json (ë¦¬ë·° 54ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_63017.json (ë¦¬ë·° 53ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_36685.json (ë¦¬ë·° 52ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_36789.json (ë¦¬ë·° 50ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_301.json (ë¦¬ë·° 50ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_98349.json (ë¦¬ë·° 52ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_68563.json (ë¦¬ë·° 52ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_238912.json (ë¦¬ë·° 20ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_340.json (ë¦¬ë·° 53ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_7565.json (ë¦¬ë·° 53ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_53238.json (ë¦¬ë·° 55ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_41660.json (ë¦¬ë·° 53ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_41501.json (ë¦¬ë·° 56ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_663.json (ë¦¬ë·° 50ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_272123.json (ë¦¬ë·° 15ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_3229.json (ë¦¬ë·° 52ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_39.json (ë¦¬ë·° 50ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_35356.json (ë¦¬ë·° 56ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_116460.json (ë¦¬ë·° 56ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_116461.json (ë¦¬ë·° 47ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_187279.json (ë¦¬ë·° 51ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_185643.json (ë¦¬ë·° 35ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_185644.json (ë¦¬ë·° 29ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_306.json (ë¦¬ë·° 50ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_96408.json (ë¦¬ë·° 58ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_38229.json (ë¦¬ë·° 164ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_9096.json (ë¦¬ë·° 157ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "ğŸ¥ƒ ë¶„ì„ ì¤‘: wb_1884.json (ë¦¬ë·° 86ê°œ)... -> âœ… ì™„ë£Œ!\n",
            "\n",
            "ğŸ‰ ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n",
            "ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜: /content/drive/MyDrive/Tjoproject_model/processed_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# gemini apië¥¼ ì´ìš©í•œ ì¦ë¥˜ì†Œëª…, ì œí’ˆëª… í•œê¸€ë²ˆì—­\n",
        "# [0] ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì„¤ì •\n",
        "# ======================================================\n",
        "# !pip install -q google-generativeai\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import google.generativeai as genai\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# 2. API í‚¤ ì„¤ì • (âš ï¸ ê¼­ ë„£ì–´ì•¼ í•©ë‹ˆë‹¤!)\n",
        "MY_API_KEY = \"\"\n",
        "genai.configure(api_key=MY_API_KEY)\n",
        "\n",
        "MODEL_NAME = 'gemini-2.5-flash-lite'\n",
        "model = genai.GenerativeModel(MODEL_NAME)\n",
        "TARGET_FOLDER = '/content/drive/MyDrive/Tjoproject_model/processed_data'\n",
        "\n",
        "# ======================================================\n",
        "# [1] í—¬í¼ í•¨ìˆ˜: í•œê¸€ í¬í•¨ ì—¬ë¶€ í™•ì¸\n",
        "# ======================================================\n",
        "def has_korean(text):\n",
        "    \"\"\"ë¬¸ìì—´ì— í•œê¸€ì´ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\"\"\"\n",
        "    if not text: return False\n",
        "    # í•œê¸€ ìœ ë‹ˆì½”ë“œ ë²”ìœ„ ì²´í¬\n",
        "    for char in text:\n",
        "        if ord('ê°€') <= ord(char) <= ord('í£'):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# ======================================================\n",
        "# [2] ë²ˆì—­ í•¨ìˆ˜\n",
        "# ======================================================\n",
        "def translate_batch(batch_data):\n",
        "    if not batch_data: return {}\n",
        "\n",
        "    prompt_text = \"Translate the following Whisky Names and Distilleries into Korean.\\n\"\n",
        "    prompt_text += \"Return ONLY a JSON list format: [{\\\"id\\\": \\\"...\\\", \\\"name_kr\\\": \\\"...\\\", \\\"dist_kr\\\": \\\"...\\\"}, ...]\\n\"\n",
        "    prompt_text += \"Rules: 1. 'Year Old' -> 'ë…„', 2. Phonetic translation (Glenfiddich -> ê¸€ë Œí”¼ë”•)\\n\\n\"\n",
        "\n",
        "    prompt_text += \"[Input Data]\\n\"\n",
        "    for item in batch_data:\n",
        "        prompt_text += f\"- ID: {item['id']} | Name: {item['name']} | Distillery: {item['dist']}\\n\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt_text)\n",
        "        clean_text = response.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "        result_list = json.loads(clean_text)\n",
        "        result_map = {item['id']: item for item in result_list if 'id' in item}\n",
        "        return result_map\n",
        "    except Exception as e:\n",
        "        print(f\"  âš ï¸ ë°°ì¹˜ API í˜¸ì¶œ ì‹¤íŒ¨ (ì ì‹œ ëŒ€ê¸° í›„ ë„˜ì–´ê°): {e}\")\n",
        "        return {}\n",
        "\n",
        "# ======================================================\n",
        "# [3] ë©”ì¸ ì‹¤í–‰ (ìˆ˜ì •ëœ ë¡œì§)\n",
        "# ======================================================\n",
        "def fix_translations():\n",
        "    json_files = glob.glob(os.path.join(TARGET_FOLDER, \"*.json\"))\n",
        "    print(f\"ğŸ“‚ íŒŒì¼ {len(json_files)}ê°œ í™•ì¸ë¨.\")\n",
        "\n",
        "    batch_queue = []\n",
        "\n",
        "    print(\"ê²€ì‚¬ ì¤‘...\", end=\"\")\n",
        "    for file_path in json_files:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            info = data.get('info', {})\n",
        "            ws_id = info.get('ws_id')\n",
        "\n",
        "            curr_name_kr = info.get('ws_name_kr', '')\n",
        "\n",
        "            # [ì¡°ê±´] 1. ë²ˆì—­ í•„ë“œê°€ ì•„ì˜ˆ ì—†ê±°ë‚˜  OR  2. ìˆëŠ”ë° í•œê¸€ì´ ì—†ì„ ë•Œ -> ì¬ì‘ì—… ëŒ€ìƒ\n",
        "            if not curr_name_kr or not has_korean(curr_name_kr):\n",
        "                item = {\n",
        "                    'id': ws_id,\n",
        "                    'name': info.get('ws_name', info.get('name', 'Unknown')),\n",
        "                    'dist': info.get('ws_distillery', info.get('distillery', 'Unknown')),\n",
        "                    'file_path': file_path\n",
        "                }\n",
        "                batch_queue.append(item)\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(f\" ì™„ë£Œ!\\nğŸš€ ì¬ë²ˆì—­ ëŒ€ìƒ: {len(batch_queue)}ê°œ / {len(json_files)}ê°œ\")\n",
        "\n",
        "    if len(batch_queue) == 0:\n",
        "        print(\"ğŸ‰ ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ì •ìƒì ìœ¼ë¡œ í•œê¸€ ë²ˆì—­ë˜ì–´ ìˆìŠµë‹ˆë‹¤!\")\n",
        "        return\n",
        "\n",
        "    # ë°°ì¹˜ ì²˜ë¦¬\n",
        "    BATCH_SIZE = 20\n",
        "\n",
        "    for i in tqdm(range(0, len(batch_queue), BATCH_SIZE), desc=\"ë³µêµ¬ ì§„í–‰ ì¤‘\"):\n",
        "        current_batch = batch_queue[i : i + BATCH_SIZE]\n",
        "\n",
        "        translated_map = translate_batch(current_batch)\n",
        "\n",
        "        # ê²°ê³¼ ì €ì¥\n",
        "        for item in current_batch:\n",
        "            target_id = item['id']\n",
        "            f_path = item['file_path']\n",
        "\n",
        "            # API ê²°ê³¼ê°€ ì—†ìœ¼ë©´(ì‹¤íŒ¨í•˜ë©´) ì €ì¥í•˜ì§€ ì•ŠìŒ (ë‹¤ìŒì— ë‹¤ì‹œ ì‹œë„í•˜ê²Œ ë‘ )\n",
        "            if target_id not in translated_map:\n",
        "                continue\n",
        "\n",
        "            trans_res = translated_map[target_id]\n",
        "            name_kr = trans_res.get('name_kr')\n",
        "            dist_kr = trans_res.get('dist_kr')\n",
        "\n",
        "            # ê²°ê³¼ê°€ ìœ íš¨í•œì§€(í•œê¸€ì¸ì§€) ì´ì¤‘ ì²´í¬\n",
        "            if not name_kr or not has_korean(name_kr):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                with open(f_path, 'r', encoding='utf-8') as f:\n",
        "                    file_data = json.load(f)\n",
        "\n",
        "                # ê°’ ì—…ë°ì´íŠ¸\n",
        "                file_data['info']['ws_name_kr'] = name_kr\n",
        "                file_data['info']['ws_distillery_kr'] = dist_kr\n",
        "\n",
        "                with open(f_path, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(file_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "        time.sleep(2)\n",
        "\n",
        "    print(\"\\nğŸ‰ ë³µêµ¬ ì™„ë£Œ! ë‹¤ì‹œ íŒŒì¼ì„ í™•ì¸í•´ë³´ì„¸ìš”.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fix_translations()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGgAH_4FTYdD",
        "outputId": "953181e8-2d73-4cc6-a3cd-f3b4cb0080b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ íŒŒì¼ 49ê°œ í™•ì¸ë¨.\n",
            "ê²€ì‚¬ ì¤‘... ì™„ë£Œ!\n",
            "ğŸš€ ì¬ë²ˆì—­ ëŒ€ìƒ: 0ê°œ / 49ê°œ\n",
            "ğŸ‰ ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ì •ìƒì ìœ¼ë¡œ í•œê¸€ ë²ˆì—­ë˜ì–´ ìˆìŠµë‹ˆë‹¤!\n"
          ]
        }
      ]
    }
  ]
}